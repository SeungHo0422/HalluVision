"""
CDF ê¸°ë°˜ í‰ê°€ í´ë˜ìŠ¤
referencesì˜ analysis_qa.ipynbì™€ eval_summary_prof.ipynbë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
from sklearn.metrics import precision_score, recall_score, f1_score


class CDFEvaluator:
    """CDF ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "cdf_analysis_output"):
        """
        Args:
            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ê·¸ë˜í”„ ì €ì¥ ë””ë ‰í† ë¦¬
        self.figures_dir = self.output_dir / "figures"
        self.figures_dir.mkdir(exist_ok=True)
        
        # NER ë¼ë²¨
        self.LABELS = ["Malware", "System", "Indicator", "Vulnerability", "Organization"]
        self.LABELS_DISPLAY = ['MAL', 'SYS', 'IND', 'VUL', 'ORG']
        
        # ëª¨ë¸ë³„ ìƒ‰ìƒ
        self.model_colors = {
            "gpt-3.5-turbo": "orange",
            "gpt-4o-mini": "green",
            "pipeline_model": "blue"
        }
    
    def calculate_entity_metrics_scores(
        self, 
        original_data: Dict[str, Any], 
        llm_results: List[Dict[str, Any]],
        metric_type: str = "f1"
    ) -> np.ndarray:
        """
        ì—”í‹°í‹° ë§¤íŠ¸ë¦­ ì ìˆ˜ ê³„ì‚° (precision, recall, f1-score ê¸°ë°˜)
        
        Args:
            original_data: Ground truth ë°ì´í„°
            llm_results: LLM ê²°ê³¼ ë°ì´í„°
            metric_type: 'precision', 'recall', 'f1' ì¤‘ í•˜ë‚˜
            
        Returns:
            np.ndarray: metrics matrix [ë¬¸ì„œìˆ˜, ë¼ë²¨ìˆ˜]
        """
        print(f"  - ì—”í‹°í‹° {metric_type.upper()} ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        # ê²°ê³¼ë¥¼ IDë³„ë¡œ ë§¤í•‘
        llm_by_id = {item["id"]: item for item in llm_results}
        
        metrics_matrix = []
        
        for doc_id in range(len(llm_results)):
            doc_id_str = str(doc_id)
            
            # Ground truth ë°ì´í„° í™•ì¸
            if doc_id_str not in original_data:
                continue
                
            original = original_data[doc_id_str]
            
            # LLM ê²°ê³¼ í™•ì¸
            if doc_id not in llm_by_id:
                continue
                
            pred_item = llm_by_id[doc_id]
            
            doc_scores = []
            
            for label in self.LABELS:
                # Ground truth ì—”í‹°í‹°
                true_entities = set(entity.lower() for entity in original.get(label, []))
                
                # ì˜ˆì¸¡ ì—”í‹°í‹° (QA ë˜ëŠ” ìš”ì•½ ê²°ê³¼ì—ì„œ ì¶”ì¶œ)
                pred_entities = set()
                if "qa_text" in pred_item:
                    # QA ê²°ê³¼ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
                    qa_text = pred_item["qa_text"].strip("{} ")
                    if qa_text and qa_text.lower() != "no answer":
                        entities = [e.strip() for e in qa_text.split(",")]
                        if len(entities) == len(self.LABELS):
                            # ìˆœì„œëŒ€ë¡œ ë§¤í•‘ (references ì½”ë“œ ì°¸ì¡°)
                            label_idx = self.LABELS.index(label)
                            if label_idx == 2:  # Indicator
                                entity = entities[3].lower()  # Vulnerabilityì™€ êµì²´
                            elif label_idx == 3:  # Vulnerability
                                entity = entities[2].lower()  # Indicatorì™€ êµì²´
                            else:
                                entity = entities[label_idx].lower()
                            
                            if entity != "no answer":
                                pred_entities.add(entity)
                
                elif "summary_text" in pred_item:
                    # ìš”ì•½ ê²°ê³¼ì—ì„œëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
                    # í˜„ì¬ëŠ” ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬
                    summary_text = pred_item.get("summary_text", "").lower()
                    for entity in true_entities:
                        if entity in summary_text:
                            pred_entities.add(entity)
                
                # Precision, Recall, F1-score ê³„ì‚°
                if len(true_entities) == 0 and len(pred_entities) == 0:
                    precision = recall = f1 = 1.0
                elif len(true_entities) == 0:
                    precision = recall = f1 = 0.0
                elif len(pred_entities) == 0:
                    precision = recall = f1 = 0.0
                else:
                    tp = len(true_entities & pred_entities)  # True Positives
                    fp = len(pred_entities - true_entities)  # False Positives  
                    fn = len(true_entities - pred_entities)  # False Negatives
                    
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
                
                # ì„ íƒëœ ë©”íŠ¸ë¦­ ë°˜í™˜
                if metric_type == "precision":
                    score = precision
                elif metric_type == "recall":
                    score = recall
                else:  # f1
                    score = f1
                
                doc_scores.append(score)
            
            metrics_matrix.append(doc_scores)
        
        return np.array(metrics_matrix)
    
    def plot_metrics_cdf(
        self, 
        original_data: Dict[str, Any],
        llm_results: List[Dict[str, Any]], 
        model_name: str, 
        plot_type: str = "qa"
    ) -> Dict[str, Any]:
        """
        Precision, Recall, F1-score CDF ê·¸ë˜í”„ ìƒì„±
        
        Args:
            original_data: Ground truth ë°ì´í„°
            llm_results: LLM ê²°ê³¼ ë°ì´í„°
            model_name: ëª¨ë¸ëª…
            plot_type: 'qa' ë˜ëŠ” 'summary'
            
        Returns:
            Dict: í†µê³„ ì •ë³´
        """
        print(f"  - {plot_type.upper()} Precision/Recall/F1 CDF ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        metrics = ["precision", "recall", "f1"]
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        all_stats = {}
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx]
            
            # ë©”íŠ¸ë¦­ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            metrics_matrix = self.calculate_entity_metrics_scores(
                original_data, llm_results, metric
            )
            
            num_labels = metrics_matrix.shape[1]
            label_median = []
            label_mean = []
            
            for i in range(num_labels):
                label_scores = metrics_matrix[:, i]
                sorted_scores = np.sort(label_scores)
                
                label_median.append(np.median(sorted_scores))
                label_mean.append(np.mean(sorted_scores))
                
                # CDF ê³„ì‚°
                cdf = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)
                
                # í”Œë¡¯
                ax.plot(sorted_scores, cdf, 
                       label=f"{self.LABELS_DISPLAY[i]} (Î¼: {np.mean(sorted_scores):.3f})",
                       linewidth=2, alpha=0.8)
            
            ax.set_xlabel(f"{metric.capitalize()} Score", fontsize=12)
            ax.set_ylabel("Cumulative Probability", fontsize=12)
            ax.set_title(f"{metric.capitalize()} CDF", fontsize=14, fontweight='bold')
            ax.legend(loc='lower right', fontsize=9)
            ax.grid(True, alpha=0.3)
            
            # "Better" í™”ì‚´í‘œ ì¶”ê°€
            ax.annotate(
                text='Better â†’',
                xy=(0.7, 0.3),
                xytext=(0.5, 0.2),
                xycoords='axes fraction',
                textcoords='axes fraction',
                arrowprops=dict(arrowstyle='->', color='red', lw=1.5),
                fontsize=10, color='red'
            )
            
            # í†µê³„ ì €ì¥
            all_stats[metric] = {
                "median_scores": {label: median for label, median in zip(self.LABELS, label_median)},
                "mean_scores": {label: mean for label, mean in zip(self.LABELS, label_mean)},
                "overall_median": np.median(label_median),
                "overall_mean": np.mean(label_mean)
            }
        
        plt.suptitle(f"{plot_type.upper()} Performance CDF - {model_name}", 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # ì €ì¥
        filename = f"{plot_type}_metrics_cdf_{model_name.replace('-', '_').replace('.', '_')}.pdf"
        filepath = self.figures_dir / filename
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        # í†µê³„ ì •ë³´ ë°˜í™˜
        stats = {
            "model_name": model_name,
            "plot_type": plot_type,
            "metrics": all_stats
        }
        
        print(f"    âœ… ì €ì¥ë¨: {filepath}")
        return stats
    
    def plot_comparative_metrics_cdf(
        self, 
        original_data: Dict[str, Any],
        model_results: Dict[str, List[Dict[str, Any]]], 
        plot_type: str = "qa"
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ CDF ê·¸ë˜í”„ (Precision, Recall, F1-score)
        
        Args:
            original_data: Ground truth ë°ì´í„°
            model_results: {model_name: llm_results} ë”•ì…”ë„ˆë¦¬
            plot_type: 'qa' ë˜ëŠ” 'summary'
            
        Returns:
            Dict: ë¹„êµ í†µê³„ ì •ë³´
        """
        print(f"  - {plot_type.upper()} ëª¨ë¸ ë¹„êµ Precision/Recall/F1 CDF ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        metrics = ["precision", "recall", "f1"]
        fig, axes = plt.subplots(len(metrics), len(self.LABELS), figsize=(20, 12))
        
        if len(metrics) == 1:
            axes = axes.reshape(1, -1)
        if len(self.LABELS) == 1:
            axes = axes.reshape(-1, 1)
        
        comparison_stats = {}
        
        for metric_idx, metric in enumerate(metrics):
            for label_idx, label in enumerate(self.LABELS):
                ax = axes[metric_idx, label_idx]
                
                for model_name, llm_results in model_results.items():
                    # ë©”íŠ¸ë¦­ ê³„ì‚°
                    metrics_matrix = self.calculate_entity_metrics_scores(
                        original_data, llm_results, metric
                    )
                    
                    if metrics_matrix.shape[1] > label_idx:
                        label_scores = metrics_matrix[:, label_idx]
                        sorted_scores = np.sort(label_scores)
                        cdf = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)
                        
                        color = self.model_colors.get(model_name, 'black')
                        ax.plot(sorted_scores, cdf, 
                               label=f"{model_name} (Î¼: {np.mean(sorted_scores):.3f})",
                               color=color, linewidth=2, alpha=0.8)
                        
                        # í†µê³„ ì €ì¥
                        if model_name not in comparison_stats:
                            comparison_stats[model_name] = {}
                        if metric not in comparison_stats[model_name]:
                            comparison_stats[model_name][metric] = {}
                        comparison_stats[model_name][metric][label] = {
                            "mean": np.mean(sorted_scores),
                            "median": np.median(sorted_scores),
                            "std": np.std(sorted_scores)
                        }
                
                ax.set_xlabel(f"{metric.capitalize()} Score", fontsize=10)
                if label_idx == 0:
                    ax.set_ylabel("Cumulative Probability", fontsize=10)
                
                title = f"{self.LABELS_DISPLAY[label_idx]}"
                if metric_idx == 0:
                    title = f"{title}\n{metric.capitalize()}"
                else:
                    title = metric.capitalize()
                
                ax.set_title(title, fontsize=11, fontweight='bold')
                ax.legend(fontsize=8)
                ax.grid(True, alpha=0.3)
        
        plt.suptitle(f"{plot_type.upper()} Performance Comparison - All Models & Metrics", 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # ì €ì¥
        filename = f"{plot_type}_comparative_metrics_cdf.pdf"
        filepath = self.figures_dir / filename
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"    âœ… ì €ì¥ë¨: {filepath}")
        return comparison_stats
    
    def generate_metrics_cdf_report(
        self, 
        original_data: Dict[str, Any],
        qa_results: Optional[List[Dict[str, Any]]] = None,
        summary_results: Optional[List[Dict[str, Any]]] = None,
        model_name: str = "pipeline_model"
    ) -> Dict[str, Any]:
        """
        ì „ì²´ Precision/Recall/F1-score CDF ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            original_data: Ground truth ë°ì´í„°
            qa_results: QA ê²°ê³¼ ë°ì´í„°
            summary_results: ìš”ì•½ ê²°ê³¼ ë°ì´í„°
            model_name: ëª¨ë¸ëª…
            
        Returns:
            Dict: ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        print(f"\nğŸ“Š {model_name} Precision/Recall/F1-score CDF ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
        print("="*60)
        
        report = {
            "model_name": model_name,
            "analysis_date": str(Path.cwd()),
            "qa_analysis": None,
            "summary_analysis": None
        }
        
        # QA ë¶„ì„
        if qa_results:
            print("\n1. QA ì„±ëŠ¥ Precision/Recall/F1 CDF ë¶„ì„:")
            qa_stats = self.plot_metrics_cdf(original_data, qa_results, model_name, "qa")
            report["qa_analysis"] = qa_stats
            
            print("   [QA ì„±ëŠ¥ ìš”ì•½]")
            for metric, metric_stats in qa_stats["metrics"].items():
                print(f"   ğŸ“Š {metric.upper()}:")
                for label, score in metric_stats["mean_scores"].items():
                    print(f"      - {label}: {score:.4f}")
                print(f"      - ì „ì²´ í‰ê· : {metric_stats['overall_mean']:.4f}")
                print()
        
        # Summary ë¶„ì„
        if summary_results:
            print("\n2. Summary ì„±ëŠ¥ Precision/Recall/F1 CDF ë¶„ì„:")
            summary_stats = self.plot_metrics_cdf(original_data, summary_results, model_name, "summary")
            report["summary_analysis"] = summary_stats
            
            print("   [Summary ì„±ëŠ¥ ìš”ì•½]")
            for metric, metric_stats in summary_stats["metrics"].items():
                print(f"   ğŸ“Š {metric.upper()}:")
                for label, score in metric_stats["mean_scores"].items():
                    print(f"      - {label}: {score:.4f}")
                print(f"      - ì „ì²´ í‰ê· : {metric_stats['overall_mean']:.4f}")
                print()
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.output_dir / f"metrics_cdf_report_{model_name.replace('-', '_')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nCDF ë¶„ì„ ì™„ë£Œ!")
        print(f"ê²°ê³¼ ì €ì¥: {self.output_dir}")
        print(f"ê·¸ë˜í”„: {self.figures_dir}")
        print(f"ë¦¬í¬íŠ¸: {report_path}")
        
        return report


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    evaluator = CDFEvaluator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("CDF Evaluator í…ŒìŠ¤íŠ¸")
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  CDF ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")


if __name__ == "__main__":
    main()
