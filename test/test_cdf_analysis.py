#!/usr/bin/env python3
"""
ê¸°ì¡´ LLM ê²°ê³¼ ë°ì´í„°ë¡œ CDF ë¶„ì„ í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import json
from pathlib import Path
from analyzer.cdf_evaluator import CDFEvaluator

def test_cdf_analysis():
    """ê¸°ì¡´ ë°ì´í„°ë¡œ CDF ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    
    print("="*60)
    print("CDF Analysis Test with Existing Data")
    print("="*60)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    ground_truth_path = "datasets/ground_truth_ner.json"
    llm_results_35_path = "datasets/llm_results_gpt3.5_turbo_250427.json"
    llm_results_4o_path = "datasets/llm_results_gpt4o_mini_2503.json"
    
    print(f"\n1. ë°ì´í„° ë¡œë”©:")
    print(f"   - Ground Truth: {ground_truth_path}")
    print(f"   - GPT-3.5-turbo: {llm_results_35_path}")
    print(f"   - GPT-4o-mini: {llm_results_4o_path}")
    
    # ë°ì´í„° ë¡œë“œ
    with open(ground_truth_path, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    with open(llm_results_35_path, 'r', encoding='utf-8') as f:
        llm_results_35 = json.load(f)
    
    with open(llm_results_4o_path, 'r', encoding='utf-8') as f:
        llm_results_4o = json.load(f)
    
    # ìƒ˜í”Œ í¬ê¸° ì œí•œ (ì „ì²´ ë°ì´í„°ëŠ” ë„ˆë¬´ í¼)
    sample_size = 500
    llm_results_35_sample = llm_results_35[:sample_size]
    llm_results_4o_sample = llm_results_4o[:sample_size]
    
    print(f"\n2. ë¶„ì„ ë°ì´í„°:")
    print(f"   - Ground Truth í•­ëª©: {len(original_data)}ê°œ")
    print(f"   - GPT-3.5-turbo ìƒ˜í”Œ: {len(llm_results_35_sample)}ê°œ")
    print(f"   - GPT-4o-mini ìƒ˜í”Œ: {len(llm_results_4o_sample)}ê°œ")
    
    # CDF í‰ê°€ì ì´ˆê¸°í™”
    evaluator = CDFEvaluator("cdf_analysis_output")
    
    # ê°œë³„ ëª¨ë¸ ë¶„ì„
    print(f"\n3. ê°œë³„ ëª¨ë¸ CDF ë¶„ì„:")
    
    # GPT-3.5-turbo ë¶„ì„
    print("\nğŸ” GPT-3.5-turbo ë¶„ì„...")
    report_35 = evaluator.generate_metrics_cdf_report(
        original_data=original_data,
        qa_results=llm_results_35_sample,
        model_name="gpt-3.5-turbo"
    )
    
    # GPT-4o-mini ë¶„ì„
    print("\nğŸ” GPT-4o-mini ë¶„ì„...")
    report_4o = evaluator.generate_metrics_cdf_report(
        original_data=original_data,
        qa_results=llm_results_4o_sample,
        model_name="gpt-4o-mini"
    )
    
    # ëª¨ë¸ ë¹„êµ ë¶„ì„
    print(f"\n4. ëª¨ë¸ ë¹„êµ Precision/Recall/F1 CDF ë¶„ì„:")
    
    # ëª¨ë¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    model_results = {
        "gpt-3.5-turbo": llm_results_35_sample,
        "gpt-4o-mini": llm_results_4o_sample
    }
    
    # ë¹„êµ CDF ê·¸ë˜í”„ ìƒì„±
    comparison_stats = evaluator.plot_comparative_metrics_cdf(
        original_data, model_results, "qa"
    )
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n5. ê²°ê³¼ ìš”ì•½:")
    print("="*40)
    
    if report_35.get("qa_analysis"):
        qa_35 = report_35["qa_analysis"]
        print(f"ğŸ“Š GPT-3.5-turbo QA:")
        for metric in ["precision", "recall", "f1"]:
            if metric in qa_35["metrics"]:
                metric_stats = qa_35["metrics"][metric]
                print(f"   ğŸ“ˆ {metric.upper()}: ì „ì²´ í‰ê·  {metric_stats['overall_mean']:.4f}")
                best_label = max(metric_stats['mean_scores'], key=metric_stats['mean_scores'].get)
                worst_label = min(metric_stats['mean_scores'], key=metric_stats['mean_scores'].get)
                print(f"      - ìµœê³ : {best_label} ({metric_stats['mean_scores'][best_label]:.4f})")
                print(f"      - ìµœì €: {worst_label} ({metric_stats['mean_scores'][worst_label]:.4f})")
    
    if report_4o.get("qa_analysis"):
        qa_4o = report_4o["qa_analysis"]
        print(f"\nğŸ“Š GPT-4o-mini QA:")
        for metric in ["precision", "recall", "f1"]:
            if metric in qa_4o["metrics"]:
                metric_stats = qa_4o["metrics"][metric]
                print(f"   ğŸ“ˆ {metric.upper()}: ì „ì²´ í‰ê·  {metric_stats['overall_mean']:.4f}")
                best_label = max(metric_stats['mean_scores'], key=metric_stats['mean_scores'].get)
                worst_label = min(metric_stats['mean_scores'], key=metric_stats['mean_scores'].get)
                print(f"      - ìµœê³ : {best_label} ({metric_stats['mean_scores'][best_label]:.4f})")
                print(f"      - ìµœì €: {worst_label} ({metric_stats['mean_scores'][worst_label]:.4f})")
    
    # ëª¨ë¸ ë¹„êµ
    if len(comparison_stats) >= 2:
        print(f"\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (F1-Score ê¸°ì¤€):")
        models = list(comparison_stats.keys())
        for label in evaluator.LABELS:
            if ("f1" in comparison_stats[models[0]] and 
                label in comparison_stats[models[0]]["f1"] and
                "f1" in comparison_stats[models[1]] and 
                label in comparison_stats[models[1]]["f1"]):
                
                score_35 = comparison_stats[models[0]]["f1"][label]["mean"]
                score_4o = comparison_stats[models[1]]["f1"][label]["mean"]
                better_model = models[0] if score_35 > score_4o else models[1]
                print(f"   - {label}: {better_model} ìš°ìˆ˜ ({max(score_35, score_4o):.4f} vs {min(score_35, score_4o):.4f})")
    
    print(f"\nâœ… CDF ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ëŠ” cdf_analysis_output/ ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
    
    return report_35, report_4o, comparison_stats

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    print("ğŸ“Š ê¸°ì¡´ LLM ê²°ê³¼ ë°ì´í„° CDF ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("ğŸ” GPT-3.5-turboì™€ GPT-4o-mini ì„±ëŠ¥ ë¹„êµ")
    
    try:
        report_35, report_4o, comparison_stats = test_cdf_analysis()
        
        print(f"\n" + "="*60)
        print("âœ… Precision/Recall/F1-score CDF ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ“Š ìƒì„±ëœ CDF ê·¸ë˜í”„:")
        print("   - qa_metrics_cdf_gpt_3_5_turbo.pdf")
        print("   - qa_metrics_cdf_gpt_4o_mini.pdf") 
        print("   - qa_comparative_metrics_cdf.pdf")
        print("ğŸ“ ë¶„ì„ ë¦¬í¬íŠ¸:")
        print("   - metrics_cdf_report_gpt_3_5_turbo.json")
        print("   - metrics_cdf_report_gpt_4o_mini.json")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
